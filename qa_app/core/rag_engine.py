from venv import logger
import torch, json, unicodedata, hashlib
import pandas as pd
import requests, re, os
import numpy as np
import openai
from collections import OrderedDict
from sentence_transformers import SentenceTransformer, util
from urllib.parse import urljoin
from qa_app.config import settings

class RAGEngine:
    def __init__(self, enable_cache: bool = True, cache_size: int = 100, semantic_cache_threshold: float = 0.95):
        """
        RAG motorunu baÅŸlatÄ±r. Modelleri ve vektÃ¶r veritabanÄ±nÄ± belleÄŸe yÃ¼kler.
        
        Args:
            enable_cache: Cache mekanizmasÄ±nÄ± aktif eder (default: True)
            cache_size: Maksimum cache boyutu (default: 100 sorgu)
            semantic_cache_threshold: Semantic cache iÃ§in minimum benzerlik skoru (default: 0.95)
        """
        print("RAG Motoru baÅŸlatÄ±lÄ±yor...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"KullanÄ±lan cihaz: {self.device}")

        # Cache ayarlarÄ±
        self.enable_cache = enable_cache
        self.cache_size = cache_size
        self._query_cache = OrderedDict()  # LRU cache iÃ§in OrderedDict (exact match)
        
        # Semantic cache
        self.semantic_cache_threshold = semantic_cache_threshold
        self._semantic_cache = []  # [(query_embedding, results), ...]
        self._semantic_cache_queries = []  # Orijinal query metinleri (debug iÃ§in)
        
        print(f"Cache: {'Aktif' if enable_cache else 'KapalÄ±'} (max {cache_size} sorgu, semantic threshold: {semantic_cache_threshold})")

        self.embedding_model = self._load_embedding_model()
        self.text_chunks, self.sources, self.embeddings = self._load_vector_db()

        # OpenAI Client Init
        self.openai_client = None
        if settings.LLM_PROVIDER == "openai":
            if not settings.OPENAI_API_KEY:
                print("UYARI: OpenAI seÃ§ildi ama OPENAI_API_KEY bulunamadÄ±!")
            else:
                self.openai_client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
                print(f"OpenAI Client baÅŸlatÄ±ldÄ± (Model: {settings.OPENAI_MODEL_NAME})")

        print("RAG Motoru baÅŸarÄ±yla baÅŸlatÄ±ldÄ± ve kullanÄ±ma hazÄ±r.")

    def _load_embedding_model(self):
        """Embedding modelini yÃ¼kler."""
        print(f"Embedding modeli yÃ¼kleniyor: {settings.EMBEDDING_MODEL}")
        return SentenceTransformer(settings.EMBEDDING_MODEL, device=self.device)

    def _load_vector_db(self):
        """Ä°ÅŸlenmiÅŸ Parquet dosyasÄ±nÄ± okur ve embedding'leri bir Torch tensor'Ã¼ne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r."""
        print(f"VektÃ¶r veritabanÄ± yÃ¼kleniyor: {settings.PROCESSED_DATA_PATH}")
        try:
            df = pd.read_parquet(settings.PROCESSED_DATA_PATH)
            text_chunks = df['text_chunk'].tolist()
            sources = df['source_document'].tolist()
            embeddings_list = df['embedding'].tolist()
            embeddings = torch.tensor(np.array(embeddings_list), dtype=torch.float32).to(self.device)
            print(f"VektÃ¶r veritabanÄ± baÅŸarÄ±yla yÃ¼klendi. Toplam {len(text_chunks)} chunk.")
            return text_chunks, sources, embeddings
        except FileNotFoundError:
            print(f"HATA: Embedding dosyasÄ± bulunamadÄ±! LÃ¼tfen Ã¶nce 'scripts/ingest.py' script'ini Ã§alÄ±ÅŸtÄ±rÄ±n.")
            raise

    def add_knowledge(self, text: str, source: str):
        """
        Dynamically adds new knowledge to the vector database (memory + disk).
        """
        try:
            print(f"Adding new knowledge from source: {source}")
            
            # 1. Compute embedding
            with torch.no_grad():
                new_embedding = self.embedding_model.encode(
                    text,
                    convert_to_tensor=True,
                    device=self.device,
                    show_progress_bar=False
                )
            
            # 2. Update In-Memory Data
            self.text_chunks.append(text)
            self.sources.append(source)
            self.embeddings = torch.cat((self.embeddings, new_embedding.unsqueeze(0)), dim=0)
            
            # 3. Update Disk (Parquet)
            # Load existing DF to append safely
            try:
                df = pd.read_parquet(settings.PROCESSED_DATA_PATH)
                new_row = pd.DataFrame([{
                    "text_chunk": text,
                    "source_document": source,
                    "embedding": new_embedding.cpu().numpy()
                }])
                df = pd.concat([df, new_row], ignore_index=True)
                df.to_parquet(settings.PROCESSED_DATA_PATH)
                print("Knowledge successfully saved to Parquet.")
                
                # 4. Invalidate Cache
                # This ensures the next query (likely the same one) doesn't hit the stale cache
                self.clear_cache()
                
            except Exception as e:
                print(f"Error saving to parquet: {e}")

        except Exception as e:
            print(f"Error adding knowledge: {e}")

    # ==================== CACHE METHODS ====================
    def _get_cache_key(self, query: str, top_k: int) -> str:
        """Sorgu iÃ§in unique cache key Ã¼retir"""
        key_str = f"{query.lower().strip()}_{top_k}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str):
        """Cache'den sonuÃ§ getirir (LRU - en son kullanÄ±lanÄ± gÃ¼ncelle)"""
        if cache_key in self._query_cache:
            # LRU: En son kullanÄ±lanÄ± en sona taÅŸÄ±
            self._query_cache.move_to_end(cache_key)
            return self._query_cache[cache_key]
        return None

    def _save_to_cache(self, cache_key: str, results: list):
        """Sonucu cache'e kaydeder (LRU mantÄ±ÄŸÄ±)"""
        if len(self._query_cache) >= self.cache_size:
            # En eski elemanÄ± sil (FIFO - OrderedDict'in ilk elemanÄ±)
            self._query_cache.popitem(last=False)
        self._query_cache[cache_key] = results

    def clear_cache(self):
        """Cache'i temizler"""
        self._query_cache.clear()
        self._semantic_cache.clear()
        self._semantic_cache_queries.clear()
        print("âœ… Cache temizlendi")

    def get_cache_stats(self) -> dict:
        """Cache istatistiklerini dÃ¶ndÃ¼rÃ¼r"""
        return {
            "enabled": self.enable_cache,
            "exact_cache_size": len(self._query_cache),
            "semantic_cache_size": len(self._semantic_cache),
            "max_size": self.cache_size,
            "semantic_threshold": self.semantic_cache_threshold
        }
    
    def _find_semantic_match(self, query_embedding):
        """Semantik olarak benzer cached sorgu var mÄ±?"""
        if not self._semantic_cache:
            return None
        
        # TÃ¼m cache embeddingler ile karÅŸÄ±laÅŸtÄ±r
        cached_embeddings = torch.stack([item[0] for item in self._semantic_cache])
        similarities = util.cos_sim(query_embedding, cached_embeddings)[0]
        
        max_idx = similarities.argmax()
        max_sim = similarities[max_idx].item()
        
        if max_sim >= self.semantic_cache_threshold:
            print(f"ğŸ’¡ Semantic cache hit! (benzerlik: {max_sim:.2%})")
            print(f"   Orijinal sorgu: '{self._semantic_cache_queries[max_idx]}'")
            return self._semantic_cache[max_idx][1]  # Cached results
        
        return None
    
    def _save_to_semantic_cache(self, query_embedding, query_text, results):
        """Semantic cache'e kaydet"""
        if len(self._semantic_cache) >= self.cache_size:
            self._semantic_cache.pop(0)
            self._semantic_cache_queries.pop(0)
        
        self._semantic_cache.append((query_embedding.cpu(), results))
        self._semantic_cache_queries.append(query_text)
    # ======================================================

    def retrieve(self, query: str, top_k: int = 5, similarity_threshold: float = 0.3, use_cache: bool = None) -> list[dict]:
        """
        Anlamsal arama yapar ve metin parÃ§alarÄ±nÄ± kaynak bilgileriyle birlikte dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            query: KullanÄ±cÄ± sorgusu
            top_k: En benzer kaÃ§ sonuÃ§ dÃ¶ndÃ¼rÃ¼lecek
            similarity_threshold: Minimum benzerlik skoru
            use_cache: Cache kullanÄ±mÄ± (None ise self.enable_cache kullanÄ±lÄ±r)
        """
        # Cache kontrolÃ¼
        if use_cache is None:
            use_cache = self.enable_cache
        
        if use_cache:
            cache_key = self._get_cache_key(query, top_k)
            cached = self._get_from_cache(cache_key)
            if cached is not None:
                print("ğŸ’¾ Cache hit!")
                return cached

        # Query expansion (GELÄ°ÅTÄ°RÄ°LMÄ°Å - v3.0)
        search_query = query
        query_lower = query.lower()
        
        # 1. Ã‡AP baÅŸvuru koÅŸullarÄ± (pozitif + negatif sorular)
        if ("Ã§ift anadal" in query_lower or "Ã§ap" in query_lower) and \
           ("koÅŸul" in query_lower or "ÅŸart" in query_lower or "nasÄ±l" in query_lower or 
            "kimler" in query_lower or "baÅŸvuramaz" in query_lower or "yapamaz" in query_lower):
            print("--- INFO: 'Ã‡AP KoÅŸullarÄ±' sorgu geniÅŸletmesi (v3.0) ---")
            search_query = f"{query} Ã‡AP baÅŸvuru koÅŸullarÄ± AGNO GANO genel not ortalamasÄ± en az kaÃ§ olmalÄ± anadal baÅŸarÄ± sÄ±rasÄ± ÅŸartÄ± kabul"
        
        # 2. Ã‡AP baÅŸarÄ±sÄ±zlÄ±k/ara sÄ±nÄ±f durumlarÄ±
        elif ("Ã§ap" in query_lower or "Ã§ift anadal" in query_lower) and \
             ("kalÄ±rsa" in query_lower or "baÅŸarÄ±sÄ±z" in query_lower or "ara sÄ±nÄ±f" in query_lower or 
              "dÃ¼ÅŸÃ¼rse" in query_lower or "etkilemez" in query_lower):
            print("--- INFO: 'Ã‡AP BaÅŸarÄ±sÄ±zlÄ±k' sorgu geniÅŸletmesi ---")
            search_query = f"{query} Ã‡AP baÅŸarÄ±sÄ±zlÄ±k mezuniyet etkilemez ana dal transkript ayrÄ± program"

        # Embedding oluÅŸtur (optimized - no gradient)
        with torch.no_grad():
            query_embedding = self.embedding_model.encode(
                search_query,
                convert_to_tensor=True,
                device=self.device,
                show_progress_bar=False
            )
        
        # Similarity search
        scores = util.dot_score(query_embedding, self.embeddings)[0]
        top_results = torch.topk(scores, k=min(top_k, len(self.embeddings)))

        # SonuÃ§larÄ± filtrele
        results = []
        for score, idx in zip(top_results.values, top_results.indices):
            if score > similarity_threshold:
                results.append({
                    "text": self.text_chunks[idx],
                    "source": self.sources[idx]
                })
        
        # Cache'e kaydet
        if use_cache:
            self._save_to_cache(cache_key, results)
        
        return results
    
    def _clean_llm_output(self, text: str) -> str:
        """LLM Ã§Ä±ktÄ±sÄ±ndaki istenmeyen tÃ¼m etiketleri ve formatlamayÄ± temizler."""
        text = unicodedata.normalize('NFKC', text).strip()
        
        label_pattern = r'.*?\b(CEVAP|YANIT|ANSWER|Ã–ZET|SONUÃ‡|NOT)\s*[:*\-â€“â€”]\s*\*{0,2}\s*'
        match = re.search(label_pattern, text, flags=re.IGNORECASE | re.DOTALL)
        
        if match:
            text = text[match.end():].strip()
            logger.debug(f"Label found and removed. Remaining text: '{text[:50]}...'")
        
        unwanted_prefixes = [
            r'^\s*\*{0,2}\s*(cevap|yanÄ±t|yanit|answer|not|Ã¶zet)\s*[:\.]*\s*\*{0,2}\s*',
            r'^\s*[-â€“â€”]\s*',
        ]
        for pattern in unwanted_prefixes:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        text = re.sub(r'\*{2,}', '', text)
        text = text.replace('*', '')
        text = re.sub(r'\s+', ' ', text)
        text = text.strip(' .:*-â€“â€”')
        
        if text and text[0].islower():
            text = text[0].upper() + text[1:]
        
        return text

    def generate(self, query: str, context: list[dict], is_web_search: bool = False) -> str:
        """Verilen sorgu ve zenginleÅŸtirilmiÅŸ baÄŸlam (context) ile cevap Ã¼retir."""
        context_str = ""
        for item in context:
            context_str += f"Kaynak: {item['source']}\nMetin: {item['text']}\n\n"

        # --- DEBUG LOGGING ---
        print("\n" + "="*40)
        mode = "WEB SEARCH" if is_web_search else "RAG"
        print(f"ğŸ” {mode} CONTEXT (Query: {query})")
        print("="*40)
        print(context_str.strip())
        print("="*40 + "\n")
        # ---------------------

        if is_web_search:
            # --- WEB SEARCH PROMPT (ESNEK) ---
            prompt = f"""
Sen bir yardÄ±mcÄ± asistansÄ±n. Web arama sonuÃ§larÄ±ndan elde edilen aÅŸaÄŸÄ±daki Context bilgisini kullanarak soruya cevap ver.

Context:
{context_str}

Soru: {query}

KURALLAR:
1. Webden gelen bilgiyi kullanarak kullanÄ±cÄ±ya en iyi cevabÄ± ver.
2. Context iÃ§indeki bilgiyi sentezle ve Ã¶zetle.
3. "NO_CONTEXT" DEME. Elindeki bilgiyle yardÄ±mcÄ± olmaya Ã§alÄ±ÅŸ.
4. EÄER aranan bÃ¶lÃ¼m/konu metinde yoksa ama "ÅŸu fakÃ¼lte altÄ±nda", "ÅŸu isimle geÃ§iyor" gibi bir aÃ§Ä±klama varsa, BU BÄ°LGÄ°YÄ° KULLANARAK CEVAP VER. (Ã–rn: Matematik -> MÃ¼hendislik ve DoÄŸa Bilimleri altÄ±ndadÄ±r gibi).
5. Tek paragraf, TÃ¼rkÃ§e, net ve anlaÅŸÄ±lÄ±r Ã¶zetle.
6. CevabÄ± DOÄRUDAN baÅŸlat.
            """
        else:
            # --- RAG PROMPT (GÃœVENLÄ°/KATI) ---
            prompt = f"""
Sen bir Ã¼niversite yÃ¶netmelik uzmanÄ±sÄ±n. SADECE aÅŸaÄŸÄ±daki Context bilgisini kullanarak soruya cevap ver.

Context:
{context_str}

Soru: {query}

Ã–NEMLÄ° KURALLAR:
1. SADECE Context iÃ§inde verilen bilgiyi kullan. Kendinden bilgi ekleme.
2. EÄŸer Context iÃ§inde sorunun cevabÄ± KESÄ°N OLARAK yoksa, SADECE "NO_CONTEXT" yaz. BaÅŸka hiÃ§bir ÅŸey yazma.
3. BaÄŸlam (Context) soruyla tamamen alakasÄ±zsa, "NO_CONTEXT" yaz.
4. "Bu metinde bilgi yok" veya "Bilmiyorum" deme, sadece "NO_CONTEXT" Ã§Ä±ktÄ±sÄ± ver.
5. CevabÄ± DOÄRUDAN baÅŸlat - "Cevap:", "YanÄ±t:" gibi baÅŸlÄ±k KULLANMA.
6. Tek paragraf, TÃ¼rkÃ§e, net ve kÄ±sa cevap ver.
            """

        payload = {
            "model": settings.LLM_MODEL,
            "prompt": prompt,
            "stream": True
        }
        
        # --- OPENAI ENTEGRASYONU ---
        if settings.LLM_PROVIDER == "openai" and self.openai_client:
            try:
                stream = self.openai_client.chat.completions.create(
                    model=settings.OPENAI_MODEL_NAME,
                    messages=[
                        {"role": "system", "content": "Sen bir Ã¼niversite yÃ¶netmelik uzmanÄ±sÄ±n."},
                        {"role": "user", "content": prompt}
                    ],
                    stream=True,
                )

                buffer = ""
                is_start_cleaned = False
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        text_chunk = chunk.choices[0].delta.content
                        
                        if not is_start_cleaned:
                            buffer += text_chunk
                            if len(buffer) >= 50: # OpenAI daha temiz dÃ¶nÃ¼yor, buffer'Ä± kÄ±sa tutabiliriz
                                cleaned_buffer = self._clean_llm_output(buffer)
                                yield cleaned_buffer
                                is_start_cleaned = True
                                buffer = ""
                        else:
                            yield text_chunk
                
                if buffer and not is_start_cleaned:
                    cleaned_buffer = self._clean_llm_output(buffer)
                    yield cleaned_buffer
                    
                return # OpenAI bitti, fonksiyondan Ã§Ä±k
                
            except Exception as e:
                logger.error(f"OpenAI HatasÄ±: {e}")
                yield f"OpenAI API ile iletiÅŸimde hata oluÅŸtu: {str(e)}"
                return

        # --- OLLAMA (ESKÄ° YÃ–NTEM) ---
        try:
            api_url = urljoin(settings.OLLAMA_URL, "/api/generate")
            response = requests.post(api_url, json=payload, stream=True, timeout=300)
            response.raise_for_status()
            
            buffer = ""
            is_start_cleaned = False
            
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    text_chunk = chunk['response']
                    
                    if not is_start_cleaned:
                        buffer += text_chunk
                        if len(buffer) >= 100:
                            cleaned_buffer = self._clean_llm_output(buffer)
                            yield cleaned_buffer
                            is_start_cleaned = True
                            buffer = ""
                    else:
                        yield text_chunk
            
            if buffer and not is_start_cleaned:
                cleaned_buffer = self._clean_llm_output(buffer)
                yield cleaned_buffer
                        
        except requests.exceptions.RequestException as e:
            print(f"Ollama API'sine baÄŸlanÄ±rken hata oluÅŸtu: {e}")
            yield "ÃœzgÃ¼nÃ¼m, yapay zeka sunucusuna baÄŸlanÄ±rken bir sorun oluÅŸtu."

    def answer_query(self, query: str) -> str:
        """TÃ¼m RAG sÃ¼recini yÃ¶netir: retrieval ve generation."""
        relevant_context = self.retrieve(query, top_k=3)
        answer = self.generate(query, relevant_context)
        return answer
    
    def answer_query_with_context(self, query: str) -> dict:
        """DeÄŸerlendirme iÃ§in hem cevabÄ± hem de kullanÄ±lan context'i dÃ¶ndÃ¼rÃ¼r."""
        relevant_context_dicts = self.retrieve(query, top_k=3)
        context_texts = [item['text'] for item in relevant_context_dicts]
        answer = self.generate(query, relevant_context_dicts)
        
        return {
            "answer": answer,
            "contexts": context_texts
        }