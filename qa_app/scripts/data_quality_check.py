"""
Veri kalitesini kontrol eden script.
Ä°ÅŸlenmiÅŸ veride sorun var mÄ± diye bakar.
"""

import pandas as pd
import numpy as np
from collections import Counter
import re

def check_data_quality(parquet_path: str):
    """Ä°ÅŸlenmiÅŸ veriyi detaylÄ± kontrol eder"""
    
    print("="*70)
    print("VERÄ° KALÄ°TE KONTROLÃœ BAÅžLATILIYOR")
    print("="*70)
    
    # 1. Load data
    df = pd.read_parquet(parquet_path)
    print(f" Veri yÃ¼klendi: {len(df)} chunk\n")
    
    # 2. Chunk boyut kontrolÃ¼
    print(" CHUNK BOYUT ANALÄ°ZÄ°")
    print("-" * 70)
    
    very_short = df[df['char_count'] < 100]
    very_long = df[df['char_count'] > 750]
    
    print(f"Ã‡ok kÄ±sa (<100 kar): {len(very_short)} chunk ({len(very_short)/len(df)*100:.1f}%)")
    print(f"Ã‡ok uzun (>750 kar): {len(very_long)} chunk ({len(very_long)/len(df)*100:.1f}%)")
    print(f"Ä°deal aralÄ±k (100-750): {len(df) - len(very_short) - len(very_long)} chunk")
    
    if len(very_short) > 0:
        print(f"\n  En kÄ±sa 3 chunk:")
        for idx, row in very_short.nsmallest(3, 'char_count').iterrows():
            print(f"   [{row['char_count']} kar] {row['text_chunk'][:80]}...")
    
    # 3. Kaynak dokÃ¼man kontrolÃ¼
    print(f"\nðŸ“š KAYNAK DOKÃœMAN ANALÄ°ZÄ°")
    print("-" * 70)
    
    source_counts = df['source_document'].value_counts()
    print(f"Benzersiz kaynak sayÄ±sÄ±: {len(source_counts)}")
    print(f"\nKaynak daÄŸÄ±lÄ±mÄ±:")
    
    for source, count in source_counts.head(10).items():
        pct = count / len(df) * 100
        bar = "" * int(pct / 2)
        print(f"  {source[:45]:45s} â”‚ {count:4d} chunks ({pct:5.1f}%) {bar}")
    
    # 4. Kaynak isim tutarlÄ±lÄ±k kontrolÃ¼
    print(f"\n  KAYNAK Ä°SÄ°M TUTARLILIK KONTROLÃœ")
    print("-" * 70)
    
    problematic_sources = []
    for source in source_counts.index:
        # T.C. varyasyonlarÄ±
        if 'T.C' in source or 'T. C' in source:
            if source not in ['T.C.', 'T.C']:
                problematic_sources.append(source)
        
        # DÃ¶kÃ¼man kodlarÄ±
        if re.search(r'\b\d{4}\b', source):
            problematic_sources.append(source)
        
        # PDF uzantÄ±larÄ±
        if '.pdf' in source.lower():
            problematic_sources.append(source)
    
    if problematic_sources:
        print(" Normalize edilmesi gereken kaynaklar:")
        for source in set(problematic_sources):
            print(f"   - {source}")
    else:
        print(" TÃ¼m kaynak isimleri tutarlÄ± gÃ¶rÃ¼nÃ¼yor")
    
    # 5. Madde numarasÄ± kontrolÃ¼
    print(f"\n MADDE NUMARASI ANALÄ°ZÄ°")
    print("-" * 70)
    
    has_madde = df[df['madde_no'].notna()]
    print(f"Madde numarasÄ± olan chunk: {len(has_madde)} ({len(has_madde)/len(df)*100:.1f}%)")
    
    if len(has_madde) > 0:
        madde_counts = Counter(has_madde['madde_no'])
        print(f"En sÄ±k gÃ¶rÃ¼len madde numaralarÄ±:")
        for madde, count in madde_counts.most_common(10):
            print(f"   Madde {madde}: {count} chunk")
    
    # 6. Section type daÄŸÄ±lÄ±mÄ±
    print(f"\n BÃ–LÃœM TÄ°PÄ° DAÄžILIMI")
    print("-" * 70)
    
    section_dist = df['section_type'].value_counts()
    total = len(df)
    
    for section, count in section_dist.items():
        pct = count / total * 100
        bar = "" * int(pct / 3)
        print(f"  {section:12s} â”‚ {count:4d} chunks ({pct:5.1f}%) {bar}")
    
    # 7. Context window kontrolÃ¼
    print(f"\n CONTEXT WINDOW ANALÄ°ZÄ°")
    print("-" * 70)
    
    no_prev_context = df[df['context_before'] == '']
    no_next_context = df[df['context_after'] == '']
    
    print(f"Ã–nceki context yok: {len(no_prev_context)} chunk")
    print(f"Sonraki context yok: {len(no_next_context)} chunk")
    print(f"Her iki context da var: {len(df) - len(no_prev_context) - len(no_next_context)} chunk")
    
    # 8. Embedding kontrolÃ¼
    print(f"\n EMBEDDING ANALÄ°ZÄ°")
    print("-" * 70)
    
    emb_shape = df['embedding'].iloc[0].shape
    print(f"Embedding boyutu: {emb_shape}")
    
    # Null embedding kontrolÃ¼
    null_embeddings = 0
    zero_embeddings = 0
    
    for emb in df['embedding'].head(100):  # Ä°lk 100'Ã¼ kontrol et
        if emb is None:
            null_embeddings += 1
        elif np.all(emb == 0):
            zero_embeddings += 1
    
    if null_embeddings > 0:
        print(f" NULL embedding: {null_embeddings}")
    if zero_embeddings > 0:
        print(f" SÄ±fÄ±r embedding: {zero_embeddings}")
    if null_embeddings == 0 and zero_embeddings == 0:
        print(f" TÃ¼m embedding'ler geÃ§erli")
    
    # 9. Duplicate kontrolÃ¼
    print(f"\nðŸ” DUPLÄ°KAT KONTROLÃœ")
    print("-" * 70)
    
    duplicate_texts = df[df.duplicated(subset=['text_chunk'], keep=False)]
    if len(duplicate_texts) > 0:
        print(f"  AynÄ± text'e sahip {len(duplicate_texts)} chunk bulundu")
        print(f"   Benzersiz duplicate text sayÄ±sÄ±: {duplicate_texts['text_chunk'].nunique()}")
    else:
        print(f" Duplicate chunk yok")
    
    # 10. Ä°Ã§erik kalitesi spot check
    print(f"\n Ä°Ã‡ERÄ°K KALÄ°TESÄ° SPOT CHECK")
    print("-" * 70)
    
    # Random 3 chunk gÃ¶ster
    sample_chunks = df.sample(min(3, len(df)))
    
    for idx, row in sample_chunks.iterrows():
        print(f"\n Ã–rnek Chunk #{idx}")
        print(f"   Kaynak: {row['source_document']}")
        print(f"   Madde: {row['madde_no'] or 'N/A'}")
        print(f"   Tip: {row['section_type']}")
        print(f"   Uzunluk: {row['char_count']} karakter, {row['word_count']} kelime")
        print(f"   Ä°Ã§erik: {row['text_chunk'][:200]}...")
    
    # 11. GENEL SKOR
    print(f"\n" + "="*70)
    print(" GENEL KALÄ°TE SKORU")
    print("="*70)
    
    score = 100
    issues = []
    
    # Chunk boyut kontrolÃ¼
    if len(very_short) / len(df) > 0.1:
        score -= 10
        issues.append("Ã‡ok fazla kÄ±sa chunk var")
    
    # Kaynak tutarlÄ±lÄ±k
    if len(problematic_sources) > 0:
        score -= 15
        issues.append("Kaynak isimleri normalize edilmeli")
    
    # Embedding kontrolÃ¼
    if null_embeddings > 0 or zero_embeddings > 0:
        score -= 20
        issues.append("GeÃ§ersiz embedding'ler var")
    
    # Duplicate kontrolÃ¼
    if len(duplicate_texts) / len(df) > 0.05:
        score -= 10
        issues.append("Ã‡ok fazla duplicate chunk")
    
    print(f"\n Kalite Skoru: {score}/100")
    
    if score >= 90:
        print(" MÃœKEMMEL! Veri kullanÄ±ma hazÄ±r.")
    elif score >= 70:
        print("  Ä°YÄ° ama iyileÅŸtirme yapÄ±labilir:")
        for issue in issues:
            print(f"   - {issue}")
    else:
        print(" SORUNLAR VAR! DÃ¼zeltme gerekli:")
        for issue in issues:
            print(f"   - {issue}")
    
    print("="*70)
    
    return {
        'total_chunks': len(df),
        'quality_score': score,
        'issues': issues,
        'very_short_chunks': len(very_short),
        'duplicate_chunks': len(duplicate_texts),
        'problematic_sources': len(problematic_sources)
    }


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        parquet_path = sys.argv[1]
    else:
        # Default path
        parquet_path = "qa_app/data/processed/gtu_rules_embeddings.parquet"
    
    try:
        result = check_data_quality(parquet_path)
        
        # Exit code: 0 if perfect, 1 if warnings, 2 if critical issues
        if result['quality_score'] >= 90:
            sys.exit(0)
        elif result['quality_score'] >= 70:
            sys.exit(1)
        else:
            sys.exit(2)
            
    except Exception as e:
        print(f"\n HATA: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(3)